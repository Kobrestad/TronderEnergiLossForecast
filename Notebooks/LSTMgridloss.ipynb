{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":140,"outputs":[{"output_type":"stream","text":"/kaggle/input/grid-loss-time-series-dataset/test_backfilled_missing_data.csv\n/kaggle/input/grid-loss-time-series-dataset/test.csv\n/kaggle/input/grid-loss-time-series-dataset/train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def shift_pandas_column(column, periods=24 * 7):\n    return column.shift(periods, fill_value=column.mean())\n\ndef get_datasets(\n    train_location=\"/kaggle/input/grid-loss-time-series-dataset/train.csv\",\n    test_location=\"/kaggle/input/grid-loss-time-series-dataset/test.csv\",\n):\n    columns_to_drop = [\n        \"grid1-loss-prophet-daily\",\n        \"grid1-loss-prophet-pred\",\n        \"grid1-loss-prophet-trend\",\n        \"grid1-loss-prophet-weekly\",\n        \"grid1-loss-prophet-yearly\",\n        \"grid2-load\",\n        \"grid2-loss\",\n        \"grid2-loss-prophet-daily\",\n        \"grid2-loss-prophet-pred\",\n        \"grid2-loss-prophet-trend\",\n        \"grid2-loss-prophet-weekly\",\n        \"grid2-loss-prophet-yearly\",\n        \"grid2_1-temp\",\n        \"grid2_2-temp\",\n        \"grid3-load\",\n        \"grid3-loss\",\n        \"grid3-loss-prophet-daily\",\n        \"grid3-loss-prophet-pred\",\n        \"grid3-loss-prophet-trend\",\n        \"grid3-loss-prophet-weekly\",\n        \"grid3-loss-prophet-yearly\",\n        \"grid3-temp\",\n    ]\n    raw_train = pd.read_csv(train_location, parse_dates=True)\n    raw_test = pd.read_csv(test_location, parse_dates=True)\n    pruned_train = raw_train.drop(columns=columns_to_drop)\n    pruned_test = raw_test.drop(columns=columns_to_drop)\n    pruned_train = raw_train.drop(columns=columns_to_drop).fillna( method='ffill')\n    pruned_test = raw_test.drop(columns=columns_to_drop).fillna( method='ffill')\n    \n\n    # get y values before shifting\n    train_y = pruned_train[\"grid1-loss\"].copy()\n    test_y = pruned_test[\"grid1-loss\"].copy()\n\n    # shift grid loss and load features 1 week to emulate real world delay of measurements\n    pruned_train[\"grid1-loss\"] = shift_pandas_column(pruned_train[\"grid1-loss\"])\n    pruned_train[\"grid1-load\"] = shift_pandas_column(pruned_train[\"grid1-load\"])\n    pruned_test[\"grid1-loss\"] = shift_pandas_column(pruned_test[\"grid1-loss\"])\n    pruned_test[\"grid1-load\"] = shift_pandas_column(pruned_test[\"grid1-load\"])\n\n    # give name to first column and change name on lagged load and loss\n    pruned_train.rename(columns={\"Unnamed: 0\": \"timestamp\"}, inplace=True)\n    pruned_test.rename(columns={\"Unnamed: 0\": \"timestamp\"}, inplace=True)\n    pruned_train.rename(columns={\"grid1-loss\": \"grid1-loss-lagged\"}, inplace=True)\n    pruned_test.rename(columns={\"grid1-loss\": \"grid1-loss-lagged\"}, inplace=True)\n    pruned_train.rename(columns={\"grid1-load\": \"grid1-load-lagged\"}, inplace=True)\n    pruned_test.rename(columns={\"grid1-load\": \"grid1-load-lagged\"}, inplace=True)\n    \n    \n    train = (pruned_train, train_y)\n    test = (pruned_test, test_y)\n\n    #return pruned_train, pruned_test\n    return train, test\n","execution_count":141,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(pruned_train, train_y), (pruned_test, test_y) = get_datasets()","execution_count":142,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\n\ndatelist_train = list(pruned_train['timestamp'])\ndatelist_train = [dt.datetime.fromisoformat(date) for date in datelist_train]\n\ndatelist_test = list(pruned_test['timestamp'])\ndatelist_test = [dt.datetime.fromisoformat(date) for date in datelist_test]","execution_count":143,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removes timestap beacuse bacause it can't be transformed to a float\nfloat_train = pruned_train.drop(columns=\"timestamp\")\ndataset_train = float_train.values.astype(float)\n\nfloat_test = pruned_test.drop(columns=\"timestamp\")\ndataset_test = float_test.values.astype(float)","execution_count":144,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#makes and fits a scalar based on the X values in the training data\nsc = StandardScaler()\ntraining_set_scaled = sc.fit_transform(dataset_train)\n\n#makes and fits a scalar based on the Y values in the training data\nscTrainY = StandardScaler()\ntraining_set_Y_scaled = scTrainY.fit_transform(train_y.values.astype(float).reshape(-1, 1))\n\n#transformes the X and the Y values of the test data based on the fitted scaler form the training data\ntesting_set_Y_scaled = scTrainY.transform(test_y.values.astype(float).reshape(-1, 1))\ntesting_set_scaled = sc.transform(dataset_test)\n","execution_count":145,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = []\ny_train = []\n\n#number of hours to predict into the future\nn_future = 36\n#number of days of each X value\nn_past = 24\n\n#makes a list of all X values, where 1 X values consists of the last \"n_past\" hours\n#makes a list of all Y values, where 1 Y value consists of 1 \"grid1-loss\", and is \"n_future\" furthere ahead that the last values in the coresponding X value\nfor i in range(n_past, len(training_set_scaled) - n_future +1):\n    X_train.append(training_set_scaled[i - n_past:i, 0:dataset_train.shape[1]])\n    y_train.append(training_set_Y_scaled[i + n_future - 1:i + n_future])\n\nX_train, y_train = np.array(X_train), np.array(y_train)\n\nprint('X_train shape == {}.'.format(X_train.shape))\nprint('y_train shape == {}.'.format(y_train.shape))\n","execution_count":146,"outputs":[{"output_type":"stream","text":"X_train shape == (17461, 24, 16).\ny_train shape == (17461, 1, 1).\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_test = []\ny_test = []\n\nfor i in range(n_past, len(testing_set_scaled) - n_future +1):\n    X_test.append(testing_set_scaled[i - n_past:i, 0:dataset_test.shape[1]])\n    y_test.append(training_set_Y_scaled[i + n_future - 1:i + n_future])\n\nX_test, y_test = np.array(X_test), np.array(y_test)\n\nprint('X_train shape == {}.'.format(X_test.shape))\nprint('y_train shape == {}.'.format(y_test.shape))","execution_count":147,"outputs":[{"output_type":"stream","text":"X_train shape == (4310, 24, 16).\ny_train shape == (4310, 1, 1).\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_combined = []\ny_combined = []\n\n#combines the test and traning data\n#this is used for adding test data to traning data later\ncombined_X_scaled = np.concatenate((training_set_scaled, testing_set_scaled))\ncombined_Y_scaled = np.concatenate((training_set_Y_scaled, testing_set_Y_scaled))\n\nfor i in range(n_past, len(combined_X_scaled) - n_future +1):\n    X_combined.append(combined_X_scaled[i - n_past:i, 0:dataset_test.shape[1] ])\n    y_combined.append(combined_Y_scaled[i + n_future - 1:i + n_future])\n\nX_combined, y_combined = np.array(X_combined), np.array(y_combined)\n\nprint('X_combined shape == {}.'.format(X_combined.shape))\nprint('y_combined shape == {}.'.format(y_combined.shape))\n\n#removes exsisting traning data\nX_combined = np.delete(X_combined, (range(17459)), axis=0)\ny_combined = np.delete(y_combined, (range(17459)), axis=0)\n\nprint('X_combined shape == {}.'.format(X_combined.shape))\nprint('y_combined shape == {}.'.format(y_combined.shape))","execution_count":148,"outputs":[{"output_type":"stream","text":"X_combined shape == (21830, 24, 16).\ny_combined shape == (21830, 1, 1).\nX_combined shape == (4371, 24, 16).\ny_combined shape == (4371, 1, 1).\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":149,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n#two LSTM layers\nmodel.add(LSTM(units=32, return_sequences=True, input_shape=(n_past, dataset_train.shape[1])))\nmodel.add(LSTM(units=8, return_sequences=False))\n#added dropout to mitigtate overfitting\nmodel.add(Dropout(0.25))\n#1 output from dens layer\nmodel.add(Dense(units=1, activation='linear'))\nmodel.compile(optimizer = Adam(learning_rate=0.001), loss='mean_squared_error')\n\n#saves initial weigths, so that these can be loaded later, when resetting the traning \nmodel.save_weights('model.inital')","execution_count":150,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trains the model with 20 % of training data used as validation data\nhistory = model.fit(X_train, y_train, shuffle=True, epochs=3, verbose=1,validation_split=0.2, batch_size=96)","execution_count":151,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n146/146 [==============================] - 1s 10ms/step - loss: 0.3616 - val_loss: 0.1201\nEpoch 2/3\n146/146 [==============================] - 1s 6ms/step - loss: 0.1893 - val_loss: 0.0949\nEpoch 3/3\n146/146 [==============================] - 1s 6ms/step - loss: 0.1546 - val_loss: 0.0702\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'][0:], label='train', color=\"blue\")\nplt.plot(history.history['val_loss'][0:], label='test', color=\"red\")","execution_count":152,"outputs":[{"output_type":"execute_result","execution_count":152,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7fbfa888e8d0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicts n_future(36) next hours for each X value in the test and traning data\npredictions_train = model.predict(X_train[n_past:])\npredictions_test = model.predict(X_test[n_past:])","execution_count":153,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#invers transfomes the predicted train and test values back to normal values\ny_pred_train = scTrainY.inverse_transform(predictions_train)\ny_pred_test = scTrainY.inverse_transform(predictions_test)","execution_count":154,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\ndef datetime_to_timestamp(x):\n    return datetime.strptime(x.strftime('%Y%m%d %H%M%S'), '%Y%m%d %H%M%S')","execution_count":155,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#makes a datafram from the predicted values with a timestamp as the index\nprediction_train = pd.DataFrame(y_pred_train, columns=['grid-loss']).set_index(pd.Series(datelist_train[2 * n_past + n_future -1:]))\nprediction_test = pd.DataFrame(y_pred_test, columns=['grid-loss']).set_index(pd.Series(datelist_test[-24 + 2 * n_past + n_future -1:-24]))\n\nprediction_train.index = prediction_train.index.to_series().apply(datetime_to_timestamp)\nprediction_test.index = prediction_test.index.to_series().apply(datetime_to_timestamp)\n\n\nactualLossTrain = pd.DataFrame(train_y.values).set_index(pd.Series(datelist_train))\nactualLossTest = pd.DataFrame(test_y.values).set_index(pd.Series(datelist_test))","execution_count":156,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allPredictions = []\n\nnumerOfIterations = 31\n\n#trains \"numerOfIterations\" number of neural networks to simulate the training of a new neural nettwork every day with new traning data \n#recomended to use GPU to run this, takes approxomently 30 min with GPU, as it is traning 170 neural nettworks\nfor i in range(numerOfIterations): #økt til resten av testdata\n\n    print(str(i+1) + \"/\" +  str(numerOfIterations))\n    #resests the weights of the model to the initial weights\n    model.load_weights('model.inital')\n    \n    #changes the verbose to 0(scilent) after 3 iterations\n    if (i>2):\n        history = model.fit(X_train, y_train, shuffle=True, epochs=10, verbose=0,validation_split=0.2, batch_size=96)\n    else:\n        history = model.fit(X_train, y_train, shuffle=True, epochs=10, verbose=2,validation_split=0.2, batch_size=96)\n    \n    #predicts the next 36 hours into the future based on the last 36 hours (n_future) starting on one week after the traning data\n    #the windows moves 24 hours per day\n    predictions_future = model.predict(X_test[24*7-n_future + 24*i:24*7 + 24*i])\n    \n    y_pred_future = scTrainY.inverse_transform(predictions_future)\n    \n    #sets the time index of the data, which is one week ahead of traning data \n    predictions_future_tmp = pd.DataFrame(y_pred_future, columns=['grid-loss']).set_index(pd.Series(datelist_test[7*24 + 24*i:7*24+n_future + 24*i]))\n    \n    #removes the first 12 hours that was predicted, as we are interested in 12 - 36 hours into the future\n    predictions_future_tmp = predictions_future_tmp.drop(pd.Series(datelist_test[7*24 + 24*i:7*24+12 + 24*i]))\n    allPredictions.append(predictions_future_tmp)\n    \n    #append 24 hours to training data\n    X_train = np.concatenate((X_train, X_combined[24*i : 24*i + 24]))\n    y_train = np.concatenate((y_train, y_combined[24*i : 24*i + 24]))\n    \n    \n\npredictions_future = pd.concat(allPredictions)\n","execution_count":null,"outputs":[{"output_type":"stream","text":"1/31\nEpoch 1/10\n146/146 - 1s - loss: 0.3509 - val_loss: 0.1142\nEpoch 2/10\n146/146 - 1s - loss: 0.1869 - val_loss: 0.0892\nEpoch 3/10\n146/146 - 1s - loss: 0.1535 - val_loss: 0.0696\nEpoch 4/10\n146/146 - 1s - loss: 0.1371 - val_loss: 0.0643\nEpoch 5/10\n146/146 - 1s - loss: 0.1258 - val_loss: 0.0698\nEpoch 6/10\n146/146 - 1s - loss: 0.1191 - val_loss: 0.0731\nEpoch 7/10\n146/146 - 1s - loss: 0.1114 - val_loss: 0.0606\nEpoch 8/10\n146/146 - 1s - loss: 0.1059 - val_loss: 0.0714\nEpoch 9/10\n146/146 - 1s - loss: 0.0996 - val_loss: 0.0735\nEpoch 10/10\n146/146 - 1s - loss: 0.1000 - val_loss: 0.0691\n2/31\nEpoch 1/10\n146/146 - 1s - loss: 0.3286 - val_loss: 0.1127\nEpoch 2/10\n146/146 - 1s - loss: 0.1825 - val_loss: 0.0817\nEpoch 3/10\n146/146 - 1s - loss: 0.1547 - val_loss: 0.0816\nEpoch 4/10\n146/146 - 1s - loss: 0.1359 - val_loss: 0.0659\nEpoch 5/10\n146/146 - 1s - loss: 0.1231 - val_loss: 0.0775\nEpoch 6/10\n146/146 - 1s - loss: 0.1189 - val_loss: 0.0996\nEpoch 7/10\n146/146 - 1s - loss: 0.1117 - val_loss: 0.0702\nEpoch 8/10\n146/146 - 1s - loss: 0.1056 - val_loss: 0.0860\nEpoch 9/10\n146/146 - 1s - loss: 0.1005 - val_loss: 0.0933\nEpoch 10/10\n146/146 - 1s - loss: 0.0950 - val_loss: 0.0884\n3/31\nEpoch 1/10\n146/146 - 1s - loss: 0.3255 - val_loss: 0.1146\nEpoch 2/10\n146/146 - 1s - loss: 0.1803 - val_loss: 0.1009\nEpoch 3/10\n146/146 - 1s - loss: 0.1519 - val_loss: 0.0778\nEpoch 4/10\n146/146 - 1s - loss: 0.1362 - val_loss: 0.0652\nEpoch 5/10\n146/146 - 1s - loss: 0.1279 - val_loss: 0.0893\nEpoch 6/10\n146/146 - 1s - loss: 0.1195 - val_loss: 0.0712\nEpoch 7/10\n146/146 - 1s - loss: 0.1114 - val_loss: 0.1052\nEpoch 8/10\n146/146 - 1s - loss: 0.1063 - val_loss: 0.0723\nEpoch 9/10\n146/146 - 1s - loss: 0.0994 - val_loss: 0.1117\nEpoch 10/10\n146/146 - 1s - loss: 0.0973 - val_loss: 0.1136\n4/31\n5/31\n6/31\n7/31\n8/31\n9/31\n10/31\n11/31\n12/31\n13/31\n14/31\n15/31\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotGraph(start_date, end_date, actualLossTimeFrame, actualLossTimeFrameTest, prediction_train, predictions_future, prediction_test):\n    plt.plot(actualLossTrain.loc[start_date:end_date], label=\"ActualTrain\")\n    plt.plot(actualLossTest.loc[start_date:end_date], label=\"ActualTest\")\n\n    plt.plot(prediction_train.loc[start_date:end_date], label=\"trainPredict\")\n    plt.plot(predictions_future.loc[start_date:end_date], label=\"futurePredict\")\n    #plt.plot(prediction_test.loc[start_date:end_date], label=\"testPredict\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotGraph('2019-11-20', '2019-12-20', actualLossTimeFrame, actualLossTimeFrameTest, prediction_train, predictions_future, prediction_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotGraph('2019-12-06', '2019-12-13', actualLossTimeFrame, actualLossTimeFrameTest, prediction_train, predictions_future, prediction_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotGraph('2017-01-27', '2020-06-03', actualLossTimeFrame, actualLossTimeFrameTest, prediction_train, predictions_future, prediction_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as mae, mean_squared_error as mse, median_absolute_error as medae\n\ndef runEvaluation(y_true, y_pred):\n    \n    metrics = [mae, root_mean_squared_error, mean_absolute_percentage_error]\n    return [calc(y_true, y_pred) for calc in metrics]\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef root_mean_squared_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_date_train = '2018-02-01'\nevaluation = runEvaluation(prediction_train.loc[start_date_train:], actualLossTrain.loc[start_date_train:])\nprint(evaluation)\n\n\nstart_date_test = '2020-01-01'\nend_date_test = '2020-05-01'\nevaluationTest = runEvaluation(prediction_test.loc[start_date_test:end_date_test], actualLossTest.loc[start_date_test:end_date_test])\nprint(evaluationTest)\n\n\nstart_date_predict = str(predictions_future[0:1].index)[16:35]\nend_date_predict = str(predictions_future[-1:].index)[16:35]\nevaluationPrediction = runEvaluation(predictions_future.loc[start_date_predict:end_date_predict], actualLossTest.loc[start_date_predict:end_date_predict])\nprint(evaluationPrediction)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}