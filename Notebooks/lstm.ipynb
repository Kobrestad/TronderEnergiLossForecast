{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def shift_pandas_column(column, periods=24 * 7):\n    return column.shift(periods, fill_value=column.mean())\n\ndef get_datasets(\n    train_location=\"/kaggle/input/grid-loss-time-series-dataset/train.csv\",\n    test_location=\"/kaggle/input/grid-loss-time-series-dataset/test.csv\",\n):\n    columns_to_drop = [\n        \"grid1-loss-prophet-daily\",\n        \"grid1-loss-prophet-pred\",\n        \"grid1-loss-prophet-trend\",\n        \"grid1-loss-prophet-weekly\",\n        \"grid1-loss-prophet-yearly\",\n        \"grid2-load\",\n        \"grid2-loss\",\n        \"grid2-loss-prophet-daily\",\n        \"grid2-loss-prophet-pred\",\n        \"grid2-loss-prophet-trend\",\n        \"grid2-loss-prophet-weekly\",\n        \"grid2-loss-prophet-yearly\",\n        \"grid2_1-temp\",\n        \"grid2_2-temp\",\n        \"grid3-load\",\n        \"grid3-loss\",\n        \"grid3-loss-prophet-daily\",\n        \"grid3-loss-prophet-pred\",\n        \"grid3-loss-prophet-trend\",\n        \"grid3-loss-prophet-weekly\",\n        \"grid3-loss-prophet-yearly\",\n        \"grid3-temp\",\n    ]\n    raw_train = pd.read_csv(train_location, parse_dates=True)\n    raw_test = pd.read_csv(test_location, parse_dates=True)\n    pruned_train = raw_train.drop(columns=columns_to_drop)\n    pruned_test = raw_test.drop(columns=columns_to_drop)\n    pruned_train = raw_train.drop(columns=columns_to_drop).fillna( method='ffill')\n    pruned_test = raw_test.drop(columns=columns_to_drop).fillna( method='ffill')\n    \n\n    # get y values before shifting\n    train_y = pruned_train[\"grid1-loss\"].copy()\n    test_y = pruned_test[\"grid1-loss\"].copy()\n\n    # shift grid loss and load features 1 week to emulate real world delay of measurements\n    pruned_train[\"grid1-loss\"] = shift_pandas_column(pruned_train[\"grid1-loss\"])\n    pruned_train[\"grid1-load\"] = shift_pandas_column(pruned_train[\"grid1-load\"])\n    pruned_test[\"grid1-loss\"] = shift_pandas_column(pruned_test[\"grid1-loss\"])\n    pruned_test[\"grid1-load\"] = shift_pandas_column(pruned_test[\"grid1-load\"])\n\n    # give name to first column and change name on lagged load and loss\n    pruned_train.rename(columns={\"Unnamed: 0\": \"timestamp\"}, inplace=True)\n    pruned_test.rename(columns={\"Unnamed: 0\": \"timestamp\"}, inplace=True)\n    pruned_train.rename(columns={\"grid1-loss\": \"grid1-loss-lagged\"}, inplace=True)\n    pruned_test.rename(columns={\"grid1-loss\": \"grid1-loss-lagged\"}, inplace=True)\n    pruned_train.rename(columns={\"grid1-load\": \"grid1-load-lagged\"}, inplace=True)\n    pruned_test.rename(columns={\"grid1-load\": \"grid1-load-lagged\"}, inplace=True)\n    \n    \n    train = (pruned_train, train_y)\n    test = (pruned_test, test_y)\n\n    #return pruned_train, pruned_test\n    return train, test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(pruned_train, train_y), (pruned_test, test_y) = get_datasets()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\n\ndatelist_train = list(pruned_train['timestamp'])\ndatelist_train = [dt.datetime.fromisoformat(date) for date in datelist_train]\n\ndatelist_test = list(pruned_test['timestamp'])\ndatelist_test = [dt.datetime.fromisoformat(date) for date in datelist_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removes timestap beacuse bacause it can't be transformed to a float\nfloat_train = pruned_train.drop(columns=\"timestamp\")\ndataset_train = float_train.values.astype(float)\n\nfloat_test = pruned_test.drop(columns=\"timestamp\")\ndataset_test = float_test.values.astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#makes and fits a scalar based on the X values in the training data\nsc = StandardScaler()\ntraining_set_scaled = sc.fit_transform(dataset_train)\n\n#makes and fits a scalar based on the Y values in the training data\nscTrainY = StandardScaler()\ntraining_set_Y_scaled = scTrainY.fit_transform(train_y.values.astype(float).reshape(-1, 1))\n\n#transformes the X and the Y values of the test data based on the fitted scaler form the training data\ntesting_set_Y_scaled = scTrainY.transform(test_y.values.astype(float).reshape(-1, 1))\ntesting_set_scaled = sc.transform(dataset_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train = []\ny_train = []\n\n#number of hours to predict into the future\nn_future = 36\n#number of days of each X value\nn_past = 24\n\n#makes a list of all X values, where 1 X values consists of the last \"n_past\" hours\n#makes a list of all Y values, where 1 Y value consists of 1 \"grid1-loss\", and is \"n_future\" furthere ahead that the last values in the coresponding X value\nfor i in range(n_past, len(training_set_scaled) - n_future +1):\n    X_train.append(training_set_scaled[i - n_past:i, 0:dataset_train.shape[1]])\n    y_train.append(training_set_Y_scaled[i + n_future - 1:i + n_future])\n\nX_train, y_train = np.array(X_train), np.array(y_train)\n\nprint('X_train shape == {}.'.format(X_train.shape))\nprint('y_train shape == {}.'.format(y_train.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_test = []\ny_test = []\n\nfor i in range(n_past, len(testing_set_scaled) - n_future +1):\n    X_test.append(testing_set_scaled[i - n_past:i, 0:dataset_test.shape[1]])\n    y_test.append(training_set_Y_scaled[i + n_future - 1:i + n_future])\n\nX_test, y_test = np.array(X_test), np.array(y_test)\n\nprint('X_train shape == {}.'.format(X_test.shape))\nprint('y_train shape == {}.'.format(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_combined = []\ny_combined = []\n\n#combines the test and traning data\n#this is used for adding test data to traning data later\ncombined_X_scaled = np.concatenate((training_set_scaled, testing_set_scaled))\ncombined_Y_scaled = np.concatenate((training_set_Y_scaled, testing_set_Y_scaled))\n\nfor i in range(n_past, len(combined_X_scaled) - n_future +1):\n    X_combined.append(combined_X_scaled[i - n_past:i, 0:dataset_test.shape[1] ])\n    y_combined.append(combined_Y_scaled[i + n_future - 1:i + n_future])\n\nX_combined, y_combined = np.array(X_combined), np.array(y_combined)\n\nprint('X_combined shape == {}.'.format(X_combined.shape))\nprint('y_combined shape == {}.'.format(y_combined.shape))\n\n#removes exsisting traning data\nX_combined = np.delete(X_combined, (range(17459)), axis=0)\ny_combined = np.delete(y_combined, (range(17459)), axis=0)\n\nprint('X_combined shape == {}.'.format(X_combined.shape))\nprint('y_combined shape == {}.'.format(y_combined.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n#two LSTM layers\nmodel.add(LSTM(units=32, return_sequences=True, input_shape=(n_past, dataset_train.shape[1])))\nmodel.add(LSTM(units=8, return_sequences=False))\n#added dropout to mitigtate overfitting\nmodel.add(Dropout(0.25))\n#1 output from dens layer\nmodel.add(Dense(units=1, activation='linear'))\nmodel.compile(optimizer = Adam(learning_rate=0.001), loss='mean_squared_error')\n\n#saves initial weigths, so that these can be loaded later, when resetting the traning \nmodel.save_weights('model.inital')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trains the model with 20 % of training data used as validation data\nhistory = model.fit(X_train, y_train, shuffle=True, epochs=10, verbose=1,validation_split=0.2, batch_size=96)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'][0:], label='train', color=\"blue\")\nplt.plot(history.history['val_loss'][0:], label='test', color=\"red\")\nplt.legend(shadow=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicts n_future(36) next hours for each X value in the test and traning data\npredictions_train = model.predict(X_train[n_past:])\npredictions_test = model.predict(X_test[n_past:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#invers transfomes the predicted train and test values back to normal values\ny_pred_train = scTrainY.inverse_transform(predictions_train)\ny_pred_test = scTrainY.inverse_transform(predictions_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\ndef datetime_to_timestamp(x):\n    return datetime.strptime(x.strftime('%Y%m%d %H%M%S'), '%Y%m%d %H%M%S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#makes a datafram from the predicted values with a timestamp as the index\nprediction_train = pd.DataFrame(y_pred_train, columns=['grid-loss']).set_index(pd.Series(datelist_train[2 * n_past + n_future -1:]))\nprediction_test = pd.DataFrame(y_pred_test, columns=['grid-loss']).set_index(pd.Series(datelist_test[-24 + 2 * n_past + n_future -1:-24]))\n\nprediction_train.index = prediction_train.index.to_series().apply(datetime_to_timestamp)\nprediction_test.index = prediction_test.index.to_series().apply(datetime_to_timestamp)\n\n\nactualLossTrain = pd.DataFrame(train_y.values).set_index(pd.Series(datelist_train))\nactualLossTest = pd.DataFrame(test_y.values).set_index(pd.Series(datelist_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allPredictions = []\n\nnumerOfIterations = 170\n\n#trains \"numerOfIterations\" number of neural networks to simulate the training of a new neural nettwork every day with new traning data \n#recomended to use GPU to run this, takes approxomently 30 min with GPU, as it is traning 170 neural nettworks\nfor i in range(numerOfIterations): #økt til resten av testdata\n\n    print(str(i+1) + \"/\" +  str(numerOfIterations))\n    #resests the weights of the model to the initial weights\n    model.load_weights('model.inital')\n    \n    #changes the verbose to 0(scilent) after 3 iterations\n    if (i>2):\n        history = model.fit(X_train, y_train, shuffle=True, epochs=10, verbose=0,validation_split=0.2, batch_size=96)\n    else:\n        history = model.fit(X_train, y_train, shuffle=True, epochs=10, verbose=2,validation_split=0.2, batch_size=96)\n    \n    #predicts the next 36 hours into the future based on the last 36 hours (n_future) starting on one week after the traning data\n    #the windows moves 24 hours per day\n    predictions_future = model.predict(X_test[24*7-n_future + 24*i:24*7 + 24*i])\n    \n    y_pred_future = scTrainY.inverse_transform(predictions_future)\n    \n    #sets the time index of the data, which is one week ahead of traning data \n    predictions_future_tmp = pd.DataFrame(y_pred_future, columns=['grid-loss']).set_index(pd.Series(datelist_test[7*24 + 24*i:7*24+n_future + 24*i]))\n    \n    #removes the first 12 hours that was predicted, as we are interested in 12 - 36 hours into the future\n    predictions_future_tmp = predictions_future_tmp.drop(pd.Series(datelist_test[7*24 + 24*i:7*24+12 + 24*i]))\n    allPredictions.append(predictions_future_tmp)\n    \n    #append 24 hours to training data\n    X_train = np.concatenate((X_train, X_combined[24*i : 24*i + 24]))\n    y_train = np.concatenate((y_train, y_combined[24*i : 24*i + 24]))\n    \n    \n\npredictions_future = pd.concat(allPredictions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = \"170_2.csv\"\n\npredictions_future.to_csv(filename, index=True)\n\n#predictions_future2 = pd.read_csv(\"../input/60days/60Days.csv\",index_col = 0, parse_dates=True)\n#predictions_future2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotGraph(start_date, end_date, actualLossTrain, actualLossTest, prediction_train, predictions_future, prediction_test):\n    \n    #plt.figure(1, figsize=(28, 3))\n    \n    plt.plot(actualLossTrain.loc[start_date:end_date], label=\"ActualTrain\")\n    plt.plot(actualLossTest.loc[start_date:end_date], label=\"ActualTest\")\n\n    plt.plot(prediction_train.loc[start_date:end_date], label=\"trainPredict\")\n    plt.plot(predictions_future.loc[start_date:end_date], label=\"futurePredict\")\n    #plt.plot(prediction_test.loc[start_date:end_date], label=\"testPredict\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotPredictions(start_date, end_date, actualLossTrain, actualLossTest, prediction_train, predictions_future, prediction_test):\n    \n    plt.figure(1, figsize=(10, 3))\n    plt.title(\"Predicted loss and actual loss on test set\")\n    #plt.plot(actualLossTrain.loc[start_date:end_date], label=\"ActualTrain\")\n    plt.plot(actualLossTest.loc[start_date:end_date], label=\"ActualTest\", color=\"green\")\n\n    #plt.plot(prediction_train.loc[start_date:end_date], label=\"trainPredict\")\n    plt.plot(predictions_future.loc[start_date:end_date], label=\"PredictedTest\", color=\"orange\")\n    #plt.plot(prediction_test.loc[start_date:end_date], label=\"testPredict\")\n    plt.grid(True)\n    plt.legend(shadow=True)\n    plt.xlabel('Month (m)')\n    plt.ylabel('Grid loss (MWh)')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPredictions('2019-12-09', '2020-05-26', actualLossTrain, actualLossTest, prediction_train, predictions_future, prediction_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotGraph('2019-11-20', '2019-12-20', actualLossTrain, actualLossTest, prediction_train, predictions_future, prediction_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotGraph('2019-12-06', '2019-12-13', actualLossTrain, actualLossTest, prediction_train, predictions_future, prediction_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotGraph('2020-01-15', '2020-01-20', actualLossTrain, actualLossTest, prediction_train, predictions_future, prediction_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotGraph('2017-01-27', '2020-06-03', actualLossTrain, actualLossTest, prediction_train, predictions_future, prediction_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as mae, mean_squared_error as mse, median_absolute_error as medae\n\ndef runEvaluation(y_true, y_pred):\n    \n    metrics = [mae, root_mean_squared_error, mean_absolute_percentage_error]\n    return [calc(y_true, y_pred) for calc in metrics]\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef root_mean_squared_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_date_train = '2018-02-01'\nevaluation = runEvaluation(prediction_train.loc[start_date_train:], actualLossTrain.loc[start_date_train:])\nprint(evaluation)\n\n\nstart_date_test = '2020-01-01'\nend_date_test = '2020-05-01'\nevaluationTest = runEvaluation(prediction_test.loc[start_date_test:end_date_test], actualLossTest.loc[start_date_test:end_date_test])\nprint(evaluationTest)\n\n\nstart_date_predict = str(predictions_future[0:1].index)[16:35]\nend_date_predict = str(predictions_future[-1:].index)[16:35]\nevaluationPrediction = runEvaluation(predictions_future.loc[start_date_predict:end_date_predict], actualLossTest.loc[start_date_predict:end_date_predict])\nprint(evaluationPrediction)\n\n#print(str(predictions_future[-1:].index)[16:35])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}